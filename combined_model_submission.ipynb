{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook uses StanfordCoreNLP for preprocessing and word stemming and for extracting possible keyphrases from the input documents. The results of word stemming of StanfordCoreNLP are maybe a bit different for different versions. In case of runtime error, we may have to add specific code for exception handling for that particular document.\n",
    "\n",
    "The model also uses FastText embeddings from https://fasttext.cc/docs/en/english-vectors.html and, so download file \"wiki-news-300d-1M-subword.vec.zip\" and unzip it and keep it in the same folder. We tested for Glove and FastText embeddings and any file containing either of these two embeddings should work fine. Simply change the filename of the embedding in cell 4.\n",
    "\n",
    "Datasets used are submitted along with this notebook. Unzip them and keep them in the same folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ideas for implementation of PositionRank and using StanfordCoreNLP taken from https://github.com/ymym3412/position-rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stanfordcorenlp import StanfordCoreNLP\n",
    "import re\n",
    "\n",
    "\n",
    "class StanfordCoreNlpTokenizer(object):\n",
    "    \"\"\"Tokenizer for English using Stanford CoreNLP for tokenization.\n",
    "    This class tokenize English sentence.\n",
    "    As a default, tokenizer returns tokens whhic POS are adjective or noun.\n",
    "    Simultaneously, tokenizer returns specific pattern phrases.\n",
    "    \"\"\"\n",
    "    def __init__(self, url_or_path, port = 9000):\n",
    "        \"\"\"Initialize stanford core nlp tokenier.\n",
    "        Args:\n",
    "          url_or_path: Url string of path string of Stanford CoreNLP library.\n",
    "            Provide url string if you already stand up Stanford CoreNLP server.\n",
    "            If not, provide path to directory of library i.e. JavaLibraries/stanford-corenlp-full-2017-06-09/.\n",
    "            When you provide path of librart, Stanford CoreNLP server will be up independent of python process.\n",
    "        \"\"\"\n",
    "        self.tokenizer = StanfordCoreNLP(url_or_path, port = port)\n",
    "\n",
    "    def tokenize(self, sentence, pos_filter=[\"JJ\", \"JJR\", \"JJS\", \"NN\", \"NNS\", \"NNP\", \"NNPS\"]):\n",
    "        \"\"\"Tokenize sentence.\n",
    "        Tokenize sentence and return token list and phrase list.\n",
    "        Phrase is continuous tokens which have specific POS pattern '(adjective)*(noun)+'\n",
    "        and length are more than 3.\n",
    "        You can edit filter of token POS, default are limited to adjective and noun.\n",
    "        Args:\n",
    "          sentence: English sentence.\n",
    "          pos_filter: POS filter of token. Default are adjective and noun.\n",
    "        Returns:\n",
    "          Token list: Filterd by 'pos_filter' param.\n",
    "          Phrase list: Specific continuous tokens.\n",
    "        \"\"\"\n",
    "        tokens = self.tokenizer.pos_tag(sentence)\n",
    "        pos_tags = [self._anonymize_pos(token[1]) for token in tokens]\n",
    "        pattern = r\"J*N+\"\n",
    "        iterator = re.finditer(pattern, \"\".join(pos_tags))\n",
    "        phrases = filter(lambda x: len(x) <= 3, [[token[0] for token in tokens[match.start():match.end()]] for match in iterator])\n",
    "        phrases = [\"_\".join(phrase) for phrase in phrases]\n",
    "        return [token[0] for token in tokens if token[1] in pos_filter], phrases\n",
    "\n",
    "    def _anonymize_pos(self, pos):\n",
    "        \"\"\"Anonymize POS tags.\n",
    "        Adjective tags are replaced to 'J', noun are to 'N', and others are to 'O'.\n",
    "        \"\"\"\n",
    "        if (pos == \"JJ\") or (pos == \"JJR\") or (pos == \"JJS\"):\n",
    "            return \"J\"\n",
    "        elif (pos == \"NN\") or (pos == \"NNS\") or (pos == \"NNP\") or (pos == \"NNPS\"):\n",
    "            return \"N\"\n",
    "        else:\n",
    "            return \"O\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "StanfordCoreNLP uses a port on the system. Change port number if 9000 not free for use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = StanfordCoreNlpTokenizer(\"stanford-corenlp\", port = 9000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import stemming.porter2 as porter\n",
    "import numpy as np\n",
    "import copy\n",
    "from collections import Counter\n",
    "from scipy import spatial\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change filename if different embeddings are to be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "999995\n"
     ]
    }
   ],
   "source": [
    "def glove_embedding(filename):\n",
    "    file_glove=open(filename)\n",
    "    glove={}\n",
    "    for line in file_glove:\n",
    "        tmp=line.split()\n",
    "        word=tmp[0]\n",
    "        coefficient=np.asarray(tmp[1:], dtype='float')\n",
    "        glove[word]=coefficient\n",
    "\n",
    "    file_glove.close()\n",
    "    return glove\n",
    "\n",
    "\n",
    "#Change filename if different embeddings are to be used\n",
    "fasttext=glove_embedding('wiki-news-300d-1M-subword.vec')\n",
    "print(len(fasttext))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Theme weighted PageRank implementation using formulae from Key2vec paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def key2vec(embed_dim,embeddings,theme, sentence, tokenizer, alpha=0.85, window_size=6, num_keyphrase=10, lang=\"en\"):\n",
    "    if lang == \"en\":\n",
    "        stem = porter.stem\n",
    "    else:\n",
    "        stem = lambda word: word\n",
    "    \n",
    "    #tokenizing using StanforCoreNLP\n",
    "    original_words, phrases = tokenizer.tokenize(sentence)\n",
    "    or_theme, ph_theme=tokenizer.tokenize(theme)\n",
    "    ph_theme_new=[s.replace('_',' ') for s in ph_theme]\n",
    "    phrases_new=[s.replace('_',' ') for s in phrases]\n",
    "    original_words_new=[s.replace('_',' ') for s in original_words]\n",
    "    \n",
    "    \n",
    "    candidate_embeddings={stem(w):[] for w in phrases_new}\n",
    "    \n",
    "    #embeddings for candidate keyphrases\n",
    "    for phr in phrases_new:\n",
    "        tmp=phr.split(\" \")\n",
    "        embed_value=np.zeros(embed_dim)\n",
    "        for word in tmp:\n",
    "            try:\n",
    "                embed_value=np.add(embed_value,embeddings[word])\n",
    "            except:\n",
    "                embed_value=np.add(embed_value,[np.random.uniform(1e-5,0.1) for i in range(embed_dim)])\n",
    "        candidate_embeddings[stem(phr)]=embed_value\n",
    "\n",
    "    \n",
    "    ph_embed_dict={w: [] for w in ph_theme_new}\n",
    "    \n",
    "    \n",
    "    #generating theme vector\n",
    "    ph_theme_embeddings=[]\n",
    "    for phr in ph_theme_new:\n",
    "        tmp=phr.split(\" \")\n",
    "        embed_value=np.zeros(embed_dim)\n",
    "        for word in tmp:\n",
    "            try:\n",
    "                embed_value=np.add(embed_value,embeddings[word])\n",
    "            except:\n",
    "                embed_value=np.add(embed_value,[np.random.uniform(1e-5,0.1) for i in range(embed_dim)])\n",
    "        \n",
    "        ph_embed_dict[phr].append(embed_value)\n",
    "    \n",
    "    #averaging frequent key2vec candidate keyphrases\n",
    "    keys_stem={stem(w):[] for w in ph_theme_new}\n",
    "    \n",
    "    for w in ph_theme_new:\n",
    "        keys_stem[stem(w)].append(w)\n",
    "    keys_embed={stem(w): [] for w in ph_theme_new}\n",
    "    keys_cnt={stem(w):0 for w in ph_theme_new}\n",
    "    for n in ph_theme_new:\n",
    "        keys_cnt[stem(n)]=keys_cnt[stem(n)]+1\n",
    "    for key in keys_cnt:\n",
    "        if keys_cnt[key]>1:\n",
    "            tmp=keys_stem[key]\n",
    "            avg=np.zeros(embed_dim)\n",
    "            for w in tmp:\n",
    "                avg=np.add(ph_embed_dict[w][0],avg)\n",
    "            keys_embed[key]=np.divide(avg,len(tmp))\n",
    "        elif keys_cnt[key]==1:\n",
    "            keys_embed[key]=ph_embed_dict[keys_stem[key][0]][0]\n",
    "\n",
    "    \n",
    "    theme_vector=np.zeros(embed_dim)\n",
    "    for key in keys_embed.keys():\n",
    "        theme_vector=np.add(keys_embed[key], theme_vector)\n",
    "    \n",
    "    \n",
    "    \n",
    "    #thematic weights: candidate similarity with theme vector\n",
    "    thematic_weights={w: 0 for w in candidate_embeddings.keys()}\n",
    "    weights=[]\n",
    "    sum_weights=0\n",
    "    for candidate in candidate_embeddings.keys():\n",
    "        sim=1-spatial.distance.cosine(theme_vector, candidate_embeddings[candidate])\n",
    "        thematic_weights[candidate]=sim\n",
    "        weights.append(sim)\n",
    "        sum_weights+=sim\n",
    "\n",
    "    for key in thematic_weights.keys():\n",
    "        thematic_weights[key]=thematic_weights[key]/sum_weights \n",
    "        \n",
    "    theme_co_dict = {w: [] for w in phrases_new}\n",
    "    for phrase in theme_co_dict.keys():\n",
    "        idx=[m.start() for m in re.finditer(phrase, sentence)]\n",
    "\n",
    "        for i in idx:\n",
    "            lcnt=0\n",
    "            rcnt=len(phrase)-1\n",
    "            cnt=0\n",
    "            for lcnt in range(i,-1,-1):\n",
    "                if sentence[lcnt]==\" \":\n",
    "                    cnt+=1\n",
    "                if cnt==window_size+1:\n",
    "                    break\n",
    "            lower=sentence[lcnt:i]\n",
    "            cnt=0\n",
    "            for rcnt in range(i+len(phrase),len(sentence)-1):\n",
    "                if sentence[rcnt]==\" \":\n",
    "                    cnt+=1\n",
    "                if cnt==window_size+1:\n",
    "                    break\n",
    "            upper=sentence[i+len(phrase):rcnt]\n",
    "            for p in theme_co_dict.keys():\n",
    "                if p in lower:\n",
    "                    theme_co_dict[phrase].append(p)\n",
    "                if p in upper:\n",
    "                    theme_co_dict[phrase].append(p)\n",
    "            \n",
    "    \n",
    "    for phrase in theme_co_dict.keys():\n",
    "        temp=theme_co_dict[phrase]\n",
    "        for conn in temp:\n",
    "            if phrase not in theme_co_dict[conn]:\n",
    "                theme_co_dict[conn].append(phrase)\n",
    "    \n",
    "\n",
    "    \n",
    "    key2idx={stem(w): [] for w in theme_co_dict.keys()}\n",
    "    for key in theme_co_dict.keys():\n",
    "        if len(theme_co_dict[key])>0:\n",
    "            for phr in theme_co_dict[key]:\n",
    "                key2idx[stem(key)].append(stem(phr))\n",
    "                \n",
    "    key_occ={stem(w):[] for w in theme_co_dict.keys()}\n",
    "    for key in theme_co_dict.keys():\n",
    "        tmp=theme_co_dict[key]\n",
    "        for w in tmp:\n",
    "            key_occ[stem(key)].append(stem(w))\n",
    "    \n",
    "    for key in key2idx.keys():\n",
    "        words=key2idx[key]\n",
    "        tmp=[]\n",
    "        for w in words:\n",
    "            if w not in tmp:\n",
    "                tmp.append(w)\n",
    "        key2idx[key]=tmp\n",
    "    \n",
    "\n",
    "    words=[w for w in sentence.split(\" \")]\n",
    "    n_words=len(words)\n",
    "    \n",
    "    unstem={stem(w): \"\" for w in theme_co_dict.keys()}\n",
    "    for w in theme_co_dict.keys():\n",
    "        unstem[stem(w)]=w\n",
    "\n",
    "    n_keys=len(key2idx.keys())\n",
    "    key_score={w : 1.0/n_keys for w in key2idx.keys()}\n",
    "    out_degree={w: len(key2idx[w]) for w in key2idx.keys()}\n",
    "\n",
    "    \n",
    "    #following code contains formulae generating the graph\n",
    "    scores=[]\n",
    "    pairs=[]\n",
    "    sum_scores=0\n",
    "    lsentence=sentence.lower()\n",
    "    for key in key2idx.keys():\n",
    "        k=key2idx[key]\n",
    "        k_occ=key_occ[key]\n",
    "        score=0.0\n",
    "        for word in k:\n",
    "            if word!=key:\n",
    "                semantic=1-spatial.distance.cosine(candidate_embeddings[key], candidate_embeddings[word])\n",
    "                try:\n",
    "                    pmi=np.log(k_occ.count(word)/(lsentence.count(unstem[key].lower())*lsentence.count(unstem[word].lower())))\n",
    "                except:\n",
    "                    pmi=np.random.uniform(0,0.01)\n",
    "                score+=(semantic*pmi/out_degree[word])\n",
    "                \n",
    "                \n",
    "        key_score[key]=(1-alpha)*thematic_weights[key]+alpha*score\n",
    "\n",
    "\n",
    "        sum_scores+=key_score[key]\n",
    "    \n",
    "    #scaling the scores\n",
    "    for key in key_score.keys():\n",
    "        key_score[key]/=sum_scores\n",
    "        scores.append(key_score[key])\n",
    "        pairs.append(key)\n",
    "    mini=np.min(scores)\n",
    "    maxi=np.max(scores)\n",
    "    for key in key_score.keys():\n",
    "        key_score[key]=(key_score[key]-mini)/(maxi-mini)\n",
    "    scores,pairs=zip(*sorted(zip(scores, pairs)))\n",
    "\n",
    "    key2vec=[]\n",
    "    \n",
    "\n",
    "    for i in range(len(pairs)-1,len(pairs)-num_keyphrase-1,-1):\n",
    "        key2vec.append(unstem[pairs[i]].lower())\n",
    "    \n",
    "    #print(\"phrases_new=\"+str(len(phrases_new)))\n",
    "    #print(\"theme_co_dict=\"+str(len(theme_co_dict.keys())))\n",
    "    #print(\"key2idx=\"+str(len(key2idx.keys())))\n",
    "    #print(\"key_score=\"+str(len(key_score.keys())))\n",
    "    #dup={stem(w):0 for w in phrases_new}\n",
    "    #print(\"stemming phrases_new=\"+str(len(dup.keys())))\n",
    "    \n",
    "    \n",
    "    return key2vec,key_score,unstem\n",
    "            \n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PositionRank implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def position_rank1(embed_dim,embeddings,theme, sentence, tokenizer, alpha=0.85, window_size=6, num_keyphrase=10, lang=\"en\"):\n",
    "\n",
    "    if lang == \"en\":\n",
    "        stem = porter.stem\n",
    "    else:\n",
    "        stem = lambda word: word\n",
    "        \n",
    "    # origial words and phrase list\n",
    "    original_words, phrases = tokenizer.tokenize(sentence)\n",
    "    phrases_new=[s.replace('_',' ') for s in phrases]\n",
    "    original_words_new=[s.replace('_',' ') for s in original_words]\n",
    "    \n",
    "    # stemmed words\n",
    "    stemmed_word = [stem(word) for word in original_words]\n",
    "    unique_word_list = set([word for word in stemmed_word])\n",
    "    n = len(unique_word_list)\n",
    "    \n",
    "    adjancency_matrix = np.zeros((n, n))\n",
    "    word2idx = {w: i for i, w in enumerate(unique_word_list)}\n",
    "    p_vec = np.zeros(n)\n",
    "    \n",
    "    # store co-occurence words\n",
    "    co_occ_dict = {w: [] for w in unique_word_list}\n",
    "\n",
    "    # 1. initialize  probability vector\n",
    "    for i, w in enumerate(stemmed_word):\n",
    "        # add position score\n",
    "        p_vec[word2idx[w]] += float(1 / (i+1))\n",
    "        for window_idx in range(1, int(math.ceil(window_size / 2)+1)):\n",
    "            if i - window_idx >= 0:\n",
    "                co_list = co_occ_dict[w]\n",
    "                co_list.append(stemmed_word[i - window_idx])\n",
    "                co_occ_dict[w] = co_list\n",
    "\n",
    "            if i + window_idx < len(stemmed_word):\n",
    "                co_list = co_occ_dict[w]\n",
    "                co_list.append(stemmed_word[i + window_idx])\n",
    "                co_occ_dict[w] = co_list\n",
    "\n",
    "    # 2. create adjancency matrix from co-occurence word\n",
    "    for w, co_list in co_occ_dict.items():\n",
    "        cnt = Counter(co_list)\n",
    "        for co_word, freq in cnt.most_common():\n",
    "            adjancency_matrix[word2idx[w]][word2idx[co_word]] = freq\n",
    "\n",
    "    adjancency_matrix = adjancency_matrix / adjancency_matrix.sum(axis=0)\n",
    "    p_vec = p_vec / p_vec.sum()\n",
    "    # principal eigenvector s\n",
    "    s_vec = np.ones(n) / n\n",
    "\n",
    "    # threshold\n",
    "    lambda_val = 1.0\n",
    "    loop = 0\n",
    "    # compute final principal eigenvector\n",
    "    while lambda_val > 0.001:\n",
    "        next_s_vec = copy.deepcopy(s_vec)\n",
    "        for i, (p, s) in enumerate(zip(p_vec, s_vec)):\n",
    "            next_s = (1 - alpha) * p + alpha * (weight_total(adjancency_matrix, i, s_vec))\n",
    "            next_s_vec[i] = next_s\n",
    "        lambda_val = np.linalg.norm(next_s_vec - s_vec)\n",
    "        s_vec = next_s_vec\n",
    "        loop += 1\n",
    "        if loop > 100:\n",
    "            break\n",
    "\n",
    "    # score original words and phrases\n",
    "    #word_with_score_list = [(word, s_vec[word2idx[stem(word)]]) for word in original_words]\n",
    "    word_with_score_list=[]\n",
    "    #print(\"creation words_with_score_list=\"+str(len(word_with_score_list)))\n",
    "    #print(phrases)\n",
    "    scores=[]\n",
    "    for phrase in phrases:\n",
    "        total_score = sum([s_vec[word2idx[stem(word)]] for word in phrase.split(\"_\")])\n",
    "        word_with_score_list.append((phrase, total_score))\n",
    "        scores.append(total_score)\n",
    "    \n",
    "    pos_score={stem(w):0 for w in phrases_new}\n",
    "\n",
    "    sums=0\n",
    "    #for word in original_words:\n",
    "    #    sums+=s_vec[word2idx[stem(word)]]\n",
    "    for tup in word_with_score_list:\n",
    "        tmp=stem(tup[0]).replace(\"_\",\" \")\n",
    "\n",
    "        pos_score[tmp]=tup[1]\n",
    "        sums+=tup[1]\n",
    "\n",
    "    mini=np.min(scores)\n",
    "    maxi=np.max(scores)\n",
    "    for key in pos_score.keys():\n",
    "        pos_score[key]=(pos_score[key]-mini)/(maxi-mini)\n",
    "\n",
    "    sort_list = np.argsort([t[1] for t in word_with_score_list])\n",
    "    keyphrase_list = []\n",
    "    \n",
    "\n",
    "    stemmed_keyphrase_list = []\n",
    "    for idx in reversed(sort_list):\n",
    "        keyphrase = word_with_score_list[idx][0]\n",
    "        stemmed_keyphrase = \" \".join([stem(word) for word in keyphrase.split(\"_\")])\n",
    "        if not stemmed_keyphrase in stemmed_keyphrase_list:\n",
    "            keyphrase_list.append(keyphrase)\n",
    "            stemmed_keyphrase_list.append(stemmed_keyphrase)\n",
    "        if len(keyphrase_list) >= num_keyphrase:\n",
    "            break\n",
    "    \n",
    "    #print(\"original_words=\"+str(len(original_words)))\n",
    "    #print(\"pos_score=\"+str(len(pos_score.keys())))\n",
    "    #print(\"word_with_score_list=\"+str(len(word_with_score_list)))\n",
    "    #print(\"phrases=\"+str(len(phrases)))\n",
    "    #dup={stem(w):0 for w in phrases_new}\n",
    "    #print(\"stemming phrases_new=\"+str(len(dup.keys())))\n",
    "    return keyphrase_list,pos_score\n",
    "\n",
    "\n",
    "def weight_total(matrix, idx, s_vec):\n",
    "    return sum([(wij / matrix.sum(axis=0)[j]) * s_vec[j] for j, wij in enumerate(matrix[idx]) if not wij == 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation code\n",
    "\n",
    "Multiple same keyphrases may be printed, as the 3 models are being evaluated simultaneously and multiple models may find the same keyphrase.\n",
    "\n",
    "Change value of topn variable for getting results of other values of k (top-k). Too big a value (like 15) may get runtime error as there may not be enough keywords in the ground truth of the document to compare.\n",
    "\n",
    "Change theme variable by adding theme_pos indexes to increase the theme excerpt. Increasing theme excerpt does not effect the performance much.\n",
    "\n",
    "Change variable dataset to \"www\" to get results for WWW dataset. Change pathnames accordingly if dataset stored in any other location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found:disease map\n",
      "Found:disease map\n",
      "Found:disease map\n",
      "Found:active explor\n",
      "Found:clickthrough data\n",
      "Found:active explor\n",
      "Found:clickthrough data\n",
      "Found:active explor\n",
      "Found:clickthrough data\n",
      "Found:quicklink\n",
      "Found:data cent\n",
      "Found:data cent\n",
      "Found:data cent\n",
      "Found:social network\n",
      "Found:social network\n",
      "Found:classif\n",
      "Found:classif\n",
      "Found:co-clust\n",
      "Found:out-of-domain\n",
      "Found:incremental crawl\n",
      "Found:web forum\n",
      "Found:incremental crawl\n",
      "Found:web forum\n",
      "Found:incremental crawl\n",
      "Found:sitemap\n",
      "Found:document represent\n",
      "Found:document represent\n",
      "Found:wikipedia\n",
      "Found:document represent\n",
      "Found:wikipedia\n",
      "Found:metric learn\n",
      "Found:multi-task learn\n",
      "Found:transfer learn\n",
      "Found:learn\n",
      "Found:metric learn\n",
      "Found:transfer learn\n",
      "Found:learn\n",
      "Found:metric learn\n",
      "Found:transfer learn\n",
      "Found:logistic regress\n",
      "Found:naive bay\n",
      "Found:logistic regress\n",
      "Found:naive bay\n",
      "Found:logistic regress\n",
      "Found:naive bay\n",
      "Found:link predict\n",
      "Found:probabilistic graph\n",
      "Found:social network\n",
      "Found:link predict\n",
      "Found:probabilistic graph\n",
      "Found:social network\n",
      "Found:link predict\n",
      "Found:probabilistic graph\n",
      "Found:social network\n",
      "Found:implicit social graph\n",
      "Found:implicit social graph\n",
      "Found:implicit social graph\n",
      "Found:quasi-experimental design\n",
      "Found:quasi-experimental design\n",
      "Found:quasi-experimental design\n",
      "Found:social network\n",
      "Found:compress\n",
      "Found:social network\n",
      "Found:compress\n",
      "Found:social network\n",
      "Found:unsupervised transfer classif\n",
      "Found:unsupervised transfer classif\n",
      "Found:text categor\n",
      "Found:stochastic gradient desc\n",
      "Found:stochastic gradient desc\n",
      "Found:stochastic gradient desc\n",
      "Found:frequent pattern\n",
      "Found:frequent pattern\n",
      "Found:privaci\n",
      "Found:association rule min\n",
      "Found:inference detect\n",
      "Found:association rule min\n",
      "Found:inference detect\n",
      "Found:association rule min\n",
      "Found:inference detect\n",
      "Found:regular\n",
      "Found:max margin\n",
      "Found:multimodal data min\n",
      "Found:max margin\n",
      "Found:multimodal data min\n",
      "Found:max margin\n",
      "Found:multimodal data min\n",
      "Found:causal model\n",
      "Found:graphical model\n",
      "Found:causal model\n",
      "Found:graphical model\n",
      "Found:causal model\n",
      "Found:cluster\n",
      "Found:constraint\n",
      "Found:cluster\n",
      "Found:constraint\n",
      "Found:cluster\n",
      "Found:constraint\n",
      "Found:conditional random field\n",
      "Found:conditional random field\n",
      "Found:nested\n",
      "Found:nested\n",
      "Found:nested\n",
      "Found:sequential pattern min\n",
      "Found:sequential pattern min\n",
      "Found:link predict\n",
      "Found:link predict\n",
      "Found:link predict\n",
      "Found:graph min\n",
      "Found:memory leak\n",
      "Found:graph min\n",
      "Found:memory leak\n",
      "Found:topic model\n",
      "Found:topic model\n",
      "Found:topic model\n",
      "Found:predict\n",
      "Found:stroke predict\n",
      "Found:feature select\n",
      "Found:predict\n",
      "Found:stroke\n",
      "Found:stroke predict\n",
      "Found:concordance index\n",
      "Found:feature select\n",
      "Found:predict\n",
      "Found:stroke\n",
      "Found:contextual advertis\n",
      "Found:sensitive content detect\n",
      "Found:contextual advertis\n",
      "Found:contextual advertis\n",
      "Found:sub-document classif\n",
      "Found:privaci\n",
      "Found:privaci\n",
      "Found:privaci\n",
      "Found:statistical relational learn\n",
      "Found:frequent pattern\n",
      "Found:uncertain data\n",
      "Found:association rul\n",
      "Found:frequent pattern\n",
      "Found:uncertain data\n",
      "Found:association rul\n",
      "Found:frequent pattern\n",
      "Found:association studi\n",
      "Found:association studi\n",
      "Found:association studi\n",
      "Found:social network\n",
      "Found:social network\n",
      "Found:social network\n",
      "Found:template detect\n",
      "Found:template detect\n",
      "Found:community identif\n",
      "Found:dynamic social network\n",
      "Found:community identif\n",
      "Found:dynamic social network\n",
      "Found:information network\n",
      "Found:information network\n",
      "Found:information network\n",
      "Found:unlabeled exampl\n",
      "Found:unlabeled exampl\n",
      "Found:unlabeled exampl\n",
      "Found:nearest neighbour\n",
      "Found:nearest neighbour\n",
      "Found:relational learn\n",
      "Found:relational learn\n",
      "Found:user behavior\n",
      "Found:user behavior\n",
      "Found:user behavior\n",
      "Found:latent dirichlet alloc\n",
      "Found:sampl\n",
      "Found:latent dirichlet alloc\n",
      "Found:sampl\n",
      "Found:latent dirichlet alloc\n",
      "Found:sampl\n",
      "Found:olap\n",
      "Found:search log\n",
      "Found:search log\n",
      "Found:search log\n",
      "Found:data publish\n",
      "Found:data publish\n",
      "Found:privaci\n",
      "Found:privaci\n",
      "Found:vulner\n",
      "Found:vulner\n",
      "Found:weighted graph\n",
      "Found:network\n",
      "Found:weighted graph\n",
      "Found:compress\n",
      "Found:network\n",
      "Found:weighted graph\n",
      "Found:graph min\n",
      "Found:graph min\n",
      "Found:graph min\n",
      "Found:frequent itemset min\n",
      "Found:frequent itemset min\n",
      "Found:latent semantic analysi\n",
      "Found:structure learn\n",
      "Found:structure learn\n",
      "Found:structure learn\n",
      "Found:differential privaci\n",
      "Found:online forum\n",
      "Found:online forum\n",
      "Found:online forum\n",
      "Found:dual spars\n",
      "Found:primal spars\n",
      "Found:dual spars\n",
      "Found:primal spars\n",
      "Found:primal spars\n",
      "Found:graph databas\n",
      "Found:graph databas\n",
      "Found:graph databas\n",
      "Found:density-based clust\n",
      "Found:wikipedia\n",
      "Found:tag recommend\n",
      "Found:tensor factor\n",
      "Found:rank\n",
      "Found:tag recommend\n",
      "Found:tensor factor\n",
      "Found:rank\n",
      "Found:tag recommend\n",
      "Found:tensor factor\n",
      "Found:partial label\n",
      "Found:partial label\n",
      "Found:partial label\n",
      "Found:relational learn\n",
      "Found:social dimens\n",
      "Found:social media\n",
      "Found:relational learn\n",
      "Found:social dimens\n",
      "Found:social media\n",
      "Found:relational learn\n",
      "Found:social dimens\n",
      "Found:temporal signatur\n",
      "Found:feature select\n",
      "Found:temporal signatur\n",
      "Found:feature select\n",
      "Found:rank-order spac\n",
      "Found:topic model\n",
      "Found:topic model\n",
      "Found:topic model\n",
      "Found:tag predict\n",
      "Found:tag predict\n",
      "Found:tag predict\n",
      "Found:social network\n",
      "Found:compress\n",
      "Found:social network\n",
      "Found:compress\n",
      "Found:social network\n",
      "Found:coordinate descent method\n",
      "Found:coordinate descent method\n",
      "Found:coordinate descent method\n",
      "Found:frequent pattern\n",
      "Found:frequent pattern\n",
      "Found:frequent pattern\n",
      "Found:rare class analysi\n",
      "Found:rare class analysi\n",
      "Found:frequent episod\n",
      "Found:frequent episod\n",
      "Found:pca\n",
      "Found:concept drift\n",
      "Found:concept drift\n",
      "Found:concept drift\n",
      "Found:semi-supervised learn\n",
      "Found:semi-supervised learn\n",
      "Found:network evolut\n",
      "Found:social network\n",
      "Found:network evolut\n",
      "Found:social network\n",
      "Found:social network\n",
      "Found:social network\n",
      "Found:social network\n",
      "Found:social network\n",
      "Found:social network\n",
      "Found:community factor\n",
      "Found:blog\n",
      "Found:community factor\n",
      "Found:blog\n",
      "Found:blogospher\n",
      "Found:linear dynamical system\n",
      "Found:linear dynamical system\n",
      "Found:kalman filt\n",
      "Found:linear dynamical system\n",
      "Found:clinical tri\n",
      "Found:clinical tri\n",
      "Found:clinical tri\n",
      "Found:string\n",
      "Found:failure detect\n",
      "Found:software behavior\n",
      "Found:failure detect\n",
      "Found:software behavior\n",
      "Found:software behavior\n",
      "Found:affiliation network\n",
      "Found:affiliation network\n",
      "Found:affiliation network\n",
      "Found:evolut\n",
      "Found:social action track\n",
      "Found:social action track\n",
      "Found:multi-label classif\n",
      "Found:shared subspac\n",
      "Found:multi-label classif\n",
      "Found:shared subspac\n",
      "Found:multi-label classif\n",
      "Found:multimodal data\n",
      "Found:multimodal data\n",
      "Found:classif\n",
      "Found:co-clust\n",
      "Found:regress\n",
      "Found:active learn\n",
      "Found:supervised learn\n",
      "Found:active learn\n",
      "Found:supervised learn\n",
      "Found:active learn\n",
      "Found:topic model\n",
      "Found:topic model\n",
      "Found:visual\n",
      "Found:topic model\n",
      "Found:visual\n",
      "Found:social search\n",
      "Found:social search\n",
      "Found:cluster\n",
      "Found:xml\n",
      "Found:data stream\n",
      "Found:data stream\n",
      "Found:heterogeneous information network\n",
      "Found:topic model\n",
      "Found:heterogeneous information network\n",
      "Found:topic model\n",
      "Found:heterogeneous information network\n",
      "Found:topic model\n",
      "Found:balanced alloc\n",
      "Found:balanced alloc\n",
      "Found:balanced alloc\n",
      "Found:privacy-preserving data min\n",
      "Found:survival analysi\n",
      "Found:privacy-preserving data min\n",
      "Found:survival analysi\n",
      "Found:survival analysi\n",
      "Found:coauthor network\n",
      "Found:coauthor network\n",
      "Found:index\n",
      "Found:time seri\n",
      "Found:index\n",
      "Found:time seri\n",
      "Found:index\n",
      "Found:time seri\n",
      "Found:citation graph\n",
      "Found:topic detect\n",
      "Found:topic detect\n",
      "Found:topic detect\n",
      "Found:divers\n",
      "Found:divers\n",
      "Found:divers\n",
      "Found:divers\n",
      "Found:cluster\n",
      "Found:class distribut\n",
      "Found:class distribut\n",
      "Found:class distribut\n",
      "Found:quantif\n",
      "Found:domain adapt\n",
      "Found:domain adapt\n",
      "Found:text min\n",
      "Found:domain adapt\n",
      "Found:text min\n",
      "Found:movie search\n",
      "Found:movie search\n",
      "Found:movie search\n",
      "Found:graph\n",
      "Found:graph\n",
      "Found:graph\n",
      "Found:information extract\n",
      "Found:wikipedia\n",
      "Found:information extract\n",
      "Found:wikipedia\n",
      "Found:wikipedia\n",
      "Found:privaci\n",
      "Found:social network\n",
      "Found:privaci\n",
      "Found:social network\n",
      "Found:web classif\n",
      "Found:web classif\n",
      "Found:data clust\n",
      "Found:pairwise constraint\n",
      "Found:data clust\n",
      "Found:data clust\n",
      "Found:pairwise constraint\n",
      "Found:influence maxim\n",
      "Found:influence maxim\n",
      "Found:social network\n",
      "Found:influence maxim\n",
      "Found:social network\n",
      "Found:bipartite graph\n",
      "Found:bipartite graph\n",
      "Found:bipartite graph\n",
      "Found:subspace kernel\n",
      "Found:classif\n",
      "Found:subspace kernel\n",
      "Found:classif\n",
      "Found:subspace kernel\n",
      "Found:gradient descent method\n",
      "Found:privacy preserv\n",
      "Found:gradient descent method\n",
      "Found:privacy preserv\n",
      "Found:gradient descent method\n",
      "Found:privacy preserv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found:probabilistic databas\n",
      "Found:probabilistic databas\n",
      "Found:probabilistic databas\n",
      "Found:time seri\n",
      "Found:time seri\n",
      "Found:time seri\n",
      "Found:sensor network\n",
      "Found:sensor network\n",
      "Found:sensor network\n",
      "Found:model\n",
      "Found:model\n",
      "Found:model\n",
      "Found:causal infer\n",
      "Found:causal infer\n",
      "Found:multi-dimensional sc\n",
      "Found:multi-dimensional sc\n",
      "Found:social influ\n",
      "Found:social network\n",
      "Found:social influ\n",
      "Found:social network\n",
      "Found:online commun\n",
      "Found:social influ\n",
      "Found:social network\n",
      "Found:markov network\n",
      "Found:network reconstruct\n",
      "Found:network reconstruct\n",
      "Found:malwar\n",
      "Found:malwar\n",
      "Found:malwar\n",
      "Found:pe fil\n",
      "Found:feature select\n",
      "Found:text classif\n",
      "Found:feature select\n",
      "Found:text classif\n",
      "Found:feature select\n",
      "Found:text classif\n",
      "Found:spectral clust\n",
      "Found:spectral clust\n",
      "Found:spectral clust\n",
      "Found:controlled experi\n",
      "Found:controlled experi\n",
      "Found:controlled experi\n",
      "Found:inferior exampl\n",
      "Found:prefer\n",
      "Found:inferior exampl\n",
      "Found:prefer\n",
      "Found:inferior exampl\n",
      "Found:prefer\n",
      "Found:social network\n",
      "Found:social network\n",
      "Found:social network\n",
      "Found:density test\n",
      "Found:graph min\n",
      "Found:graph min\n",
      "Found:graph min\n",
      "Found:concise represent\n",
      "Found:concise represent\n",
      "Found:concise represent\n",
      "Found:extreme value model\n",
      "Found:extreme value model\n",
      "Found:extreme value model\n",
      "Found:social network\n",
      "Found:social network\n",
      "Found:social network\n",
      "Found:cancer\n",
      "Found:cancer\n",
      "Found:correl\n",
      "Found:social influ\n",
      "Found:social network\n",
      "Found:social influ\n",
      "Found:social network\n",
      "Found:correl\n",
      "Found:social influ\n",
      "Found:social network\n",
      "Found:pattern detect\n",
      "Found:pattern detect\n",
      "Found:folksonomi\n",
      "Found:social metadata\n",
      "Found:folksonomi\n",
      "Found:social metadata\n",
      "Found:folksonomi\n",
      "Found:social metadata\n",
      "Found:gene select\n",
      "Found:gene select\n",
      "Found:gene select\n",
      "Found:equivalence class\n",
      "Found:equivalence class\n",
      "Found:equivalence class\n",
      "Found:label spac\n",
      "Found:serendipitous learn\n",
      "Found:label spac\n",
      "Found:serendipitous learn\n",
      "Found:label spac\n",
      "Found:cliqu\n",
      "Found:behavioral target\n",
      "Found:behavioral target\n",
      "Found:behavioral target\n",
      "Found:hierarchical clust\n",
      "Found:relative constraint\n",
      "Found:relative constraint\n",
      "Found:relative constraint\n",
      "Found:classif\n",
      "Found:classif\n",
      "Found:classif\n",
      "Found:low-rank approxim\n",
      "Found:low-rank approxim\n",
      "Found:active learn\n",
      "Found:empirical risk minim\n",
      "Found:semi-supervised learn\n",
      "Found:active learn\n",
      "Found:empirical risk minim\n",
      "Found:semi-supervised learn\n",
      "Found:active learn\n",
      "Found:empirical risk minim\n",
      "Found:semi-supervised learn\n",
      "Found:webpage understand\n",
      "Found:webpage understand\n",
      "Found:chang\n",
      "Found:hierarchical summari\n",
      "Found:parsimonious explan\n",
      "Found:chang\n",
      "Found:hierarchical summari\n",
      "Found:parsimonious explan\n",
      "Found:chang\n",
      "Found:hierarchical summari\n",
      "Found:parsimonious explan\n",
      "Found:trajectory pattern\n",
      "Found:trajectory pattern\n",
      "Found:data stream\n",
      "Found:data stream\n",
      "Found:data stream\n",
      "Found:display ad\n",
      "Found:display ad\n",
      "Found:display ad\n",
      "Found:analytics servic\n",
      "Found:analytics servic\n",
      "Found:news cycl\n",
      "Found:news cycl\n",
      "Found:feature select\n",
      "Found:feature select\n",
      "Found:feature select\n",
      "Found:stabil\n",
      "Found:cost-sensitive learn\n",
      "Found:data acquisit\n",
      "Found:cost-sensitive learn\n",
      "Found:data acquisit\n",
      "Found:cost-sensitive learn\n",
      "Found:data acquisit\n",
      "Found:multi-label classif\n",
      "Found:hypergraph\n",
      "Found:least squar\n",
      "Found:multi-label classif\n",
      "Found:hypergraph\n",
      "Found:least squar\n",
      "Found:multi-label classif\n",
      "Found:partial ord\n",
      "Found:matrix factor\n",
      "Found:model order select\n",
      "Found:matrix factor\n",
      "Found:model order select\n",
      "Found:matrix factor\n",
      "Found:relational clust\n",
      "Found:relational clust\n",
      "Found:relational clust\n",
      "Found:recommender system\n",
      "Found:recommender system\n",
      "Found:propag\n",
      "Found:propag\n",
      "Found:entity discoveri\n",
      "Found:transfer learn\n",
      "Found:transfer learn\n",
      "Found:transfer learn\n",
      "Found:evolutionary spectral clust\n",
      "Found:evolutionary spectral clust\n",
      "Found:evolutionary spectral clust\n",
      "Found:temporal smooth\n",
      "Found:heterogeneous information network\n",
      "Found:classif\n",
      "Found:heterogeneous information network\n",
      "Found:rank\n",
      "Found:classif\n",
      "Found:heterogeneous information network\n",
      "Found:rank\n",
      "Found:k-nn classif\n",
      "Found:k-nn classif\n",
      "Found:decision tre\n",
      "Found:decision tre\n",
      "Found:decision tre\n",
      "Found:community evolut\n",
      "Found:multi-mode network\n",
      "Found:community evolut\n",
      "Found:multi-mode network\n",
      "Found:community evolut\n",
      "Found:multi-mode network\n",
      "Found:cluster\n",
      "Found:cluster\n",
      "Found:distribut\n",
      "Found:distribut\n",
      "Found:distribut\n",
      "Found:multinomial distribut\n",
      "Found:event summar\n",
      "Found:event summar\n",
      "Found:event summar\n",
      "Found:rule weight\n",
      "Found:rule weight\n",
      "Found:biological network\n",
      "Found:biological network\n",
      "Found:biological network\n",
      "Found:object-level search\n",
      "Found:object-level search\n",
      "Found:object-level search\n",
      "Found:data stream\n",
      "Found:tree\n",
      "Found:data stream\n",
      "Found:tree\n",
      "Found:data stream\n",
      "Found:tree\n",
      "Found:ensemble prun\n",
      "Found:ensemble prun\n",
      "Found:ensemble prun\n",
      "Found:matrix decomposit\n",
      "Found:matrix decomposit\n",
      "Found:matrix decomposit\n",
      "Found:differential privaci\n",
      "Found:differential privaci\n",
      "Found:pet\n",
      "Found:kernel\n",
      "Found:graph\n",
      "Found:graph\n",
      "Found:graph\n",
      "Found:malicious web sit\n",
      "Found:malicious web sit\n",
      "Found:malicious web sit\n",
      "Found:mobile recommender system\n",
      "Found:mobile recommender system\n",
      "Found:pattern discoveri\n",
      "Found:feature select\n",
      "Found:feature select\n",
      "Found:feature select\n",
      "Found:discrimin\n",
      "Found:discrimin\n",
      "Found:topic model\n",
      "Found:topic model\n",
      "Found:topic model\n",
      "Found:top-k frequent item\n",
      "Found:top-k frequent item\n",
      "Found:closed episod\n",
      "Found:closed episod\n",
      "Found:closed episod\n",
      "Found:transactional databas\n",
      "Found:transactional databas\n",
      "Found:transactional databas\n",
      "Found:author predict\n",
      "Found:human mobl\n",
      "Found:social network\n",
      "Found:human mobl\n",
      "Found:social network\n",
      "Found:social network\n",
      "Found:association analysi\n",
      "Found:microarray data\n",
      "Found:association analysi\n",
      "Found:association analysi\n",
      "Found:information genealog\n",
      "Found:information genealog\n",
      "Found:document categor\n",
      "Found:entity identif\n",
      "Found:document categor\n",
      "Found:entity identif\n",
      "Found:document categor\n",
      "Found:entity identif\n",
      "Found:privaci\n",
      "Found:privaci\n",
      "Found:util\n",
      "Found:privaci\n",
      "Found:util\n",
      "Found:anomali\n",
      "Found:pattern\n",
      "Found:anomali\n",
      "Found:pattern\n",
      "Found:anomali\n",
      "Found:pattern\n",
      "Found:self-similar\n",
      "Found:bounce r\n",
      "Found:bounce r\n",
      "Found:bounce r\n",
      "Found:particle filt\n",
      "Found:particle filt\n",
      "Found:particle filt\n",
      "Found:bypass r\n",
      "Found:query abandon\n",
      "Found:bypass r\n",
      "Found:query abandon\n",
      "Found:query abandon\n",
      "Found:large-scale network\n",
      "Found:social influence analysi\n",
      "Found:large-scale network\n",
      "Found:social influence analysi\n",
      "Found:approximation algorithm\n",
      "Found:dynamic social network\n",
      "Found:approximation algorithm\n",
      "Found:dynamic social network\n",
      "Found:approximation algorithm\n",
      "Found:dynamic social network\n",
      "Found:search sess\n",
      "Found:search sess\n",
      "Found:consensus\n",
      "Found:consensus\n",
      "Found:consensus\n",
      "Found:collective classif\n",
      "Found:collective classif\n",
      "Found:collective classif\n",
      "Found:motif\n",
      "Found:motif\n",
      "Found:motif\n",
      "---------------\n",
      "No of files=297\n",
      "---------------\n",
      "PositionRank\n",
      "10.016835016835017\n",
      "20.203735144312393\n",
      "13.393359594822735\n",
      "---------------\n",
      "Combined\n",
      "10.521885521885523\n",
      "21.222410865874362\n",
      "14.068655036578503\n",
      "---------------\n",
      "Key2vec\n",
      "9.595959595959595\n",
      "19.35483870967742\n",
      "12.830613393359593\n"
     ]
    }
   ],
   "source": [
    "def evaluate(ranked, label, found, total):\n",
    "    for x in label:\n",
    "        if x in ranked:\n",
    "            print(\"Found:\"+x)\n",
    "            found += 1\n",
    "    total += len(label)\n",
    "    return found,total\n",
    "\n",
    "import os\n",
    "dataset=\"kdd\"\n",
    "\n",
    "kdd_list = os.listdir(dataset+'/contentsubset/')\n",
    "data_dir = dataset+\"/contentsubset/\"\n",
    "label_dir=dataset+\"/gold/\"\n",
    "found = 0\n",
    "total = 0\n",
    "input1 = 0\n",
    "topn = 8\n",
    "cnt=0\n",
    "found1=0\n",
    "total1=0\n",
    "found2=0\n",
    "total2=0\n",
    "for file in kdd_list:\n",
    "    #print(input)\n",
    "    f = open(data_dir+file, 'r')\n",
    "    test = f.read()\n",
    "    f.close()\n",
    "    cnt+=1\n",
    "    if len(test)>250:\n",
    "        input1 += 1\n",
    "        f = open(label_dir+file, 'r')\n",
    "        label = f.read().strip().lower().split('\\n')\n",
    "        f.close()\n",
    "        theme_pos=test.split(\".\")\n",
    "        theme=theme_pos[0]+\".\"#+theme_pos[1]+\".\"\n",
    "\n",
    "        key2,key_score,unstem=key2vec(300,fasttext,theme, test, tokenizer, 0.85, 5, topn)\n",
    "        pos_rank,pos_score=position_rank1(300,fasttext,theme, test, tokenizer, 0.85, 5, topn)\n",
    "\n",
    "        sum_scores=0\n",
    "        new_score={w:0 for w in key_score.keys()}\n",
    "        for key in key_score.keys():\n",
    "            sum_scores+=key_score[key]+pos_score[key]\n",
    "            new_score[key]=key_score[key]+pos_score[key]\n",
    "        scores,phrases=[],[]\n",
    "        for key in new_score.keys():\n",
    "            new_score[key]/=sum_scores\n",
    "            scores.append(new_score[key])\n",
    "            phrases.append(key)\n",
    "        \n",
    "        scores,phrases=zip(*sorted(zip(scores, phrases)))\n",
    "        ranked=[]\n",
    "        #print(phrases)\n",
    "        for i in range(len(phrases)-1,len(phrases)-topn-1,-1):     \n",
    "            ranked.append(phrases[i].lower())\n",
    "            \n",
    "        label=[porter.stem(w.lower()) for w in label]\n",
    "        key2=[porter.stem(w.lower()) for w in key2]\n",
    "        pos_rank=[porter.stem(w.lower().replace('_',' ')) for w in pos_rank]\n",
    "\n",
    "        found,total=evaluate(pos_rank,label,found,total)\n",
    "\n",
    "        found1,total1=evaluate(ranked,label,found1,total1)\n",
    "\n",
    "        found2, total2=evaluate(key2, label, found2, total2)\n",
    "        \n",
    "\n",
    "        \n",
    "try:\n",
    "    precise = float(found) / input1 / topn\n",
    "    recall = float(found) / total\n",
    "    f1 = 2 / (1.0 / recall + 1.0 / precise)\n",
    "    precise1=float(found1) / input1 / topn\n",
    "    recall1 = float(found1) / total1\n",
    "    f2 = 2 / (1.0 / recall1 + 1.0 / precise1)\n",
    "    precise2=float(found2) / input1 / topn\n",
    "    recall2 = float(found2) / total2\n",
    "    f3 = 2 / (1.0 / recall2 + 1.0 / precise2)\n",
    "except:\n",
    "    None\n",
    "print(\"---------------\")\n",
    "print(\"No of files=\"+str(input1))\n",
    "print(\"---------------\")\n",
    "print(\"PositionRank\")\n",
    "print(precise*100)\n",
    "print(recall*100)\n",
    "print(f1*100)\n",
    "print(\"---------------\")\n",
    "print(\"Combined\")\n",
    "print(precise1*100)\n",
    "print(recall1*100)\n",
    "print(f2*100)\n",
    "print(\"---------------\")\n",
    "print(\"Key2vec\")\n",
    "print(precise2*100)\n",
    "print(recall2*100)\n",
    "print(f3*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 648x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "n_groups = 4\n",
    "\n",
    "#WWW\n",
    "posrank=(7.0,10.5,12.1,12.3)\n",
    "key2=(8.2,11.3,11.5,11.5)\n",
    "combined=(10.1,12.8,13.0,12.5)\n",
    "\n",
    "#KDD\n",
    "posrank1 = (7.3, 10.6, 11.6, 12.1)#posrank\n",
    "key21 = (8.4,12.1,13.3,12.9)#key2vec\n",
    "combined1=(12.4,15.3,15.0,14.1)#combined\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(9,6))\n",
    "index = np.arange(n_groups)\n",
    "bar_width = 0.25\n",
    "opacity = 0.8\n",
    "\n",
    "rects1 = plt.bar(index, posrank, bar_width, \n",
    "alpha=opacity,\n",
    "color='b',\n",
    "label='PositionRank')\n",
    "\n",
    "rects2 = plt.bar(index + bar_width, key2, bar_width,\n",
    "alpha=opacity,\n",
    "color='g',\n",
    "label='Key2vec')\n",
    "\n",
    "rects3 = plt.bar(index + 2*bar_width, combined, bar_width,\n",
    "alpha=opacity,\n",
    "color='r',\n",
    "label='Combined')\n",
    "\n",
    "plt.xlabel('Top n keyphrases')\n",
    "plt.ylabel('F1 scores')\n",
    "plt.title('WWW')\n",
    "plt.xticks(index + bar_width, ('2', '4', '6', '8'))\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
